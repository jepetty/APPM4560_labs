\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}

%SetFonts

%SetFonts


\title{APPM 4560 Laboratory 1 Report}
\author{Rhys Olsen\\
\texttt{rhys.olsen@colorado.edu}
 \and Jessica Petty\\
 \texttt{jessica.petty@colorado.edu}
 }
\date{September 26, 2016}

\begin{document}
\maketitle
\section{Simulating Random Permutations}
\subsection{}

The following algorithm will simulate a random permutation of values $\left\{1,..., n \right\}$:

STEP 1: Simulate and store \textit{n} random variables $U\sim Uniform(0,1)$

STEP 2: Define some \textit{f} that maps each $Uniform\left(0,1 \right)$ random variable to the index at which it was created

STEP 3: Order the $ Uniform\left(0,1 \right)$ random variables in decreasing order

STEP 4: Apply \textit{f} to each \textit{U} to produce the permutation of integers

\subsection{}
In this situation, \textit{X} has a \textit{Binomial} distribution because it is looking at a fixed number of independent trials, each with possible outcome "success" or "failure". Additionally, the probability of success is fixed and the same for each trial. Additionally, because we are examining a permutation $\sigma$, the probability of success is $p = \frac{1}{7!} $, and the probability of failure is therefore $(1-p) = (1 - \frac{1}{7!})$.\\ \break
Now, given that \textit{X} has a \textit{Binomial} distribution, we know that $E[X] = np$, but we can also prove this fact using the definition of expected value:\\

$$E[X] = \sum_{k=0}^{n}kP(X=k)=\sum_{k=0}^{n}k\binom{n}{k}p^k(1-p)^{n-k}=\sum_{k=0}^{n}k \frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}=\sum_{k=1}^{n}\frac{n!}{(k-1)!(n-k)!}p^k(1-p)^{n-k}$$\\
because we can remove the term corresponding to $k=0$. Now, if we introduce two new variables, \textit{j} and \textit{m} such that $j=k-1$ and $m=n-1$ then we have:
$$E[X]=\sum_{j=0}^{m}\frac{(m+1)!}{j!(m-j)!}p^{j+1}(1-p)^{m-j}=(m+1)p\sum_{j=0}^m\frac{m!}{j!(m-j)!}p^j(1-p)^{m-j}$$\\
Now, remembering the Binomial Theorem:\\
$$(a+b)^m=\sum_{y=0}^{m}\frac{m!}{y!(m-y)!}a^yb^{m-y}$$\\
we get the following result when we let $a=p$ and $b=1-p$:\\
$$\sum_{j=0}^{m}\frac{m!}{y!(m-y)!}p^j(1-p)^{m-j}=\sum_{y=0}^{m}\frac{m!}{y!(m-y)!}a^yb^{m-y}=(a+b)^m=(p-(1-p))^m=1^m=1$$\\
and therefore we can conclude that\\
$$E[X]=(m+1)p\cdot 1=np$$\\

\subsection{}
\subsection{}
\subsection{}

\section{Computing Performance of Retrieving Algorithms}
\subsection{}
\subsection{}
\subsection{}
\subsection{}
\subsection{}



\end{document}  